# ============================================
# RAG í•µì‹¬ íŒŒì´í”„ë¼ì¸ (Chroma ë²„ì „)
# - ë¬¸ì„œ ë¡œë“œ (ì›¹ + CSV)
# - ì²­í¬ ë¶„í• 
# - ì„ë² ë”© & ë²¡í„°ìŠ¤í† ì–´ (Chroma persist)
# - ë¦¬íŠ¸ë¦¬ë²„ & í”„ë¡¬í”„íŠ¸ & LLM ì²´ì¸ êµ¬ì„±
# - ask() í•¨ìˆ˜ë¡œ ì™¸ë¶€ì— ì œê³µ
# ============================================

from __future__ import annotations
import os, time
from pathlib import Path
from datetime import datetime
from typing import Any, Dict, List, Tuple

import pandas as pd
import bs4
from dotenv import load_dotenv

# LangChain ê´€ë ¨ ëª¨ë“ˆ
from langchain import hub
from langchain_core.documents import Document
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.document_loaders import WebBaseLoader
from langchain_chroma import Chroma
from langchain_core.runnables import RunnablePassthrough, RunnableLambda
from langchain_core.output_parsers import StrOutputParser
from langchain_openai import ChatOpenAI, OpenAIEmbeddings

import tiktoken # í† í° ê¸¸ì´ ê³„ì‚°ì— ì‚¬ìš©

# .env ë¡œë“œ (OPENAI_API_KEY, OPENAI_MODEL ë“± í™˜ê²½ ë³€ìˆ˜ ì‚¬ìš©)
load_dotenv()

# ------------------- ê²½ë¡œ ì„¤ì • -------------------
BASE_DIR = Path(__file__).resolve().parent.parent  # backend/
DATA_DIR = BASE_DIR / "data"
CHROMA_DIR = BASE_DIR / "chroma_db"  # Chroma persist ë””ë ‰í„°ë¦¬

# ìµœê·¼ Në…„ì¹˜ë§Œ ì‚¬ìš© (ê¸°ë³¸: ìµœê·¼ 5ë…„ â†’ 2021ë…„ë¶€í„°)
FROM_YEAR = int(os.getenv("RAG_FROM_YEAR", datetime.now().year - 4))

# ì„ë² ë”© ëª¨ë¸/ë°°ì¹˜
EMBED_MODEL = os.getenv("OPENAI_EMBED_MODEL", "text-embedding-3-small")
# EMBED_BATCH = int(os.getenv("OPENAI_EMBED_BATCH", "16"))  # â˜… í•œ ë²ˆì— ë³´ë‚´ëŠ” ë¬¸ì„œ ìˆ˜(ì‘ê²Œ!)
# ì•ˆì „ í† í° ìƒí•œ(ìš”ì²­ë‹¹ 300k ì œí•œë³´ë‹¤ ì—¬ìœ  ìˆê²Œ)
TOKEN_CAP_PER_REQUEST = int(os.getenv("OPENAI_EMBED_TOKEN_CAP", "240000"))

# í† í° ê¸¸ì´ í•¨ìˆ˜ (text-embedding-3-*ì™€ í˜¸í™˜ë˜ëŠ” cl100k_base)
_enc = tiktoken.get_encoding("cl100k_base")
def _tok_len(s: str) -> int:
    return len(_enc.encode(s))

# ------------------- ë°ì´í„° ë¡œë”© -------------------
def _load_web_docs() -> List[Document]:
    """í•œì„±ëŒ€ ë„ì„œê´€ ì›¹ ë¬¸ì„œ ë¡œë“œ"""
    url = "https://hsel.hansung.ac.kr/intro_data.mir"
    loader = WebBaseLoader(
        web_path=(url,),
        bs_kwargs={"parse_only": bs4.SoupStrainer("div", attrs={"id": "intro_rule"})},
    )
    return loader.load()

def _load_bookloan_docs() -> List[Document]:
    """10ë…„ì¹˜ ë„ì„œ ëŒ€ì¶œ ë°ì´í„° ë¡œë“œ (ìµœê·¼ 5ë…„ìœ¼ë¡œ ì œí•œ)"""
    xlsx = DATA_DIR / "BookLoan_10years_data.xlsx"
    csv  = DATA_DIR / "BookLoan_10years_data.csv"

    if xlsx.exists():
        df = pd.read_excel(xlsx)
        df.to_csv(csv, index=False, encoding="utf-8-sig")
    else:
        df = pd.read_csv(csv, encoding="utf-8")

    # === [ì¶”ê°€] ìµœê·¼ 5ë…„ í•„í„° ===
    df["ëŒ€ì¶œì¼ì"] = pd.to_datetime(df["ëŒ€ì¶œì¼ì"], errors="coerce")
    df = df[df["ëŒ€ì¶œì¼ì"].dt.year >= FROM_YEAR].copy()

    # ëŒ€ì¶œì›” ì»¬ëŸ¼
    df["ëŒ€ì¶œì›”"] = df["ëŒ€ì¶œì¼ì"].dt.to_period("M").astype(str)

    # ì›”ë³„ Document ìƒì„± (í•„ìš” ì»¬ëŸ¼ë§Œ ê°„ê²°íˆ)
    docs: List[Document] = []
    for month, group in df.groupby("ëŒ€ì¶œì›”"):
        rows = group.apply(
            lambda row: (
                f"[ëŒ€ì¶œì¼ì]{row['ëŒ€ì¶œì¼ì'].date()} | "
                f"[í•™ë²ˆ]{row['í•™ë²ˆ']} | "
                f"[ì„œëª…]{row['ì„œëª…']} | "
                f"[ì €ì]{row['ì €ì']} | "
                f"[ì²­êµ¬ê¸°í˜¸]{row['ì²­êµ¬ê¸°í˜¸']} | "
                f"[ë“±ë¡ë²ˆí˜¸]{row['ë“±ë¡ë²ˆí˜¸']} | "
                f"[ì—°ì¥]{row['ì—°ì¥íšŸìˆ˜']}"
            ),
            axis=1,
        )
        docs.append(Document(page_content="\n".join(rows), metadata={"month": month}))
    return docs

# ------------------- ì²­í¬ ë¶„í• (í† í° ê¸°ì¤€) -------------------
def _split_docs(all_docs: List[Document]) -> List[Document]:
    """í† í° ê¸°ì¤€ìœ¼ë¡œ ì•ˆì • ë¶„í• : ìš”ì²­ë‹¹ í† í° ì´ˆê³¼ ë°©ì§€ì— í•µì‹¬"""
    splitter = RecursiveCharacterTextSplitter(
        chunk_size=600,       # â˜… 400~800 ì‚¬ì´ ê¶Œì¥
        chunk_overlap=60,     # â˜… 10% ì •ë„
        length_function=_tok_len,
    )
    return splitter.split_documents(all_docs)

# ------------------- ì•ˆì „ ë°°ì¹˜ ì—…ë¡œë“œ ìœ í‹¸ -------------------
def _batch_indices(n: int, batch_size: int) -> List[Tuple[int, int]]:
    return [(i, min(i + batch_size, n)) for i in range(0, n, batch_size)]

def _build_index_in_safe_batches(vs: Chroma, docs: List[Document], embedding: OpenAIEmbeddings):
    """
    Chroma ì»¬ë ‰ì…˜ì— ë¬¸ì„œë¥¼ 'ì•ˆì „ ë°°ì¹˜'ë¡œ ë‚˜ëˆ  ì¶”ê°€.
    - ë°°ì¹˜ í¬ê¸°(ë¬¸ì„œ ìˆ˜) + ë°°ì¹˜ ë‚´ í† í° ì´í•©ì„ ë™ì‹œì— ì œí•œ
    - OpenAI ì„ë² ë”© APIì˜ ìš”ì²­ë‹¹ 300k í† í° ì œí•œ íšŒí”¼
    """
    texts = [d.page_content for d in docs]
    metadatas = [d.metadata for d in docs]
    n = len(texts)
    print(f"ğŸ“¦ ì•ˆì „ ë°°ì¹˜ ì—…ë¡œë“œ: ì´ {n} ì²­í¬")

    start = 0
    while start < n:
        # 1ì°¨: ë¬¸ì„œ ìˆ˜ ê¸°ì¤€ìœ¼ë¡œ ìë¥´ê¸°
        end = min(start + EMBED_BATCH, n)

        # 2ì°¨: í† í° ìƒí•œ ê³ ë ¤í•´ì„œ end ì¡°ì •
        tok_sum = 0
        safe_end = start
        for i in range(start, end):
            t = texts[i]
            tok = _tok_len(t)
            if tok_sum + tok > TOKEN_CAP_PER_REQUEST:
                break
            tok_sum += tok
            safe_end = i + 1

        # ë§Œì•½ í•œ ë¬¸ì„œê°€ ë„ˆë¬´ ê¸¸ì–´ capì„ ë°”ë¡œ ë„˜ëŠ”ë‹¤ë©´, ê·¸ ë¬¸ì„œë¥¼ ë” ì˜ê²Œ ìª¼ê°œì•¼ í•¨
        if safe_end == start:
            # ë¹„ì •ìƒì ìœ¼ë¡œ ê¸´ ì²­í¬ê°€ ìˆë‹¤ë©´ í•˜ë“œ ì»· (ë§ˆì§€ë§‰ ì•ˆì „ì¥ì¹˜)
            t = texts[start]
            # ì•ìª½ 3000 í† í° ì •ë„ë§Œ ì„ë² ë”©(í•„ìš”í•˜ë©´ ë” ì‘ê²Œ)
            hard_cut = 3000
            truncated = _enc.decode(_enc.encode(t)[:hard_cut])
            vs.add_texts([truncated], metadatas=[metadatas[start]])
            print(f"âš ï¸ ê¸´ ì²­í¬ í•˜ë“œì»· ì²˜ë¦¬ (index {start}, ~{hard_cut} tok)")
            start += 1
            continue

        # ì•ˆì „ êµ¬ê°„ ì¶”ê°€
        vs.add_texts(texts[start:safe_end], metadatas=metadatas[start:safe_end])
        print(f"  â†’ ì—…ë¡œë“œ {start}:{safe_end} (â‰ˆ{tok_sum} tokens)")
        start = safe_end

# ------------------- ë²¡í„°ìŠ¤í† ì–´ ì¤€ë¹„ -------------------
def _get_or_create_vectorstore(splits: List[Document]) -> Chroma:
    embedding = OpenAIEmbeddings(model=EMBED_MODEL)

    if not CHROMA_DIR.exists():
        print("ğŸ”¨ ìƒˆ Chroma ì¸ë±ìŠ¤ ìƒì„± (API í˜¸ì¶œì€ ì´ë²ˆ 1íšŒ)...")
        # ë¹ˆ ì»¬ë ‰ì…˜ì„ ë§Œë“¤ê³  â†’ ì•ˆì „ ë°°ì¹˜ë¡œ add_texts
        vs = Chroma(embedding_function=embedding,
                     persist_directory=str(CHROMA_DIR))
        _build_index_in_safe_batches(
            vs, splits, embedding)
        vs.persist()
        print("âœ… ì¸ë±ìŠ¤ ìƒì„± & ì €ì¥ ì™„ë£Œ")
    else:
        print("ğŸ“ ê¸°ì¡´ Chroma ì¸ë±ìŠ¤ ë¡œë“œ (API í˜¸ì¶œ ì—†ìŒ)")
        vs = Chroma(embedding_function=embedding,
                     persist_directory=str(CHROMA_DIR))
    return vs

# ------------------- ì „ì—­ ì´ˆê¸°í™” -------------------
print("\nğŸš€ ì¸ë±ìŠ¤ ì¤€ë¹„ ì¤‘...")
web_docs = _load_web_docs()
csv_docs = _load_bookloan_docs()
ALL_DOCS = web_docs + csv_docs
SPLITS = _split_docs(ALL_DOCS)
print(f"ğŸ“„ ë¬¸ì„œ {len(ALL_DOCS)}ê°œ â†’ ì²­í¬ {len(SPLITS)}ê°œ")

VECTORSTORE = _get_or_create_vectorstore(SPLITS)

# ì›” í•„í„° (í˜„ì¬ ì›”ë§Œ ê²€ìƒ‰)
current_month = datetime.now().strftime("%Y-%m")
def _filter_by_month(docs: List[Document]) -> List[Document]:
    return [d for d in docs if d.metadata.get("month") == current_month]

RETRIEVER = VECTORSTORE.as_retriever(search_type="mmr", search_kwargs={"k": 10})
FILTERED_RETRIEVER = RETRIEVER | RunnableLambda(_filter_by_month)

# í”„ë¡¬í”„íŠ¸ & LLM ì¤€ë¹„
try:
    PROMPT = hub.pull("rlm/rag-prompt")
except Exception:
    from langchain_core.prompts import PromptTemplate
    PROMPT = PromptTemplate.from_template(
        "ë‹¤ìŒ ì»¨í…ìŠ¤íŠ¸ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë‹µí•˜ì„¸ìš”.\n\n{context}\n\nì§ˆë¬¸: {question}"
    )

MODEL_NAME = os.getenv("OPENAI_MODEL", "gpt-3.5-turbo")
LLM = ChatOpenAI(model=MODEL_NAME, temperature=0)

RAG_CHAIN = (
    {"context": RETRIEVER, "question": RunnablePassthrough()}
    | PROMPT
    | LLM
    | StrOutputParser()
)

print("ğŸ¯ RAG íŒŒì´í”„ë¼ì¸ ì¤€ë¹„ ì™„ë£Œ!\n")

# ------------------- ê³µê°œ í•¨ìˆ˜ -------------------
def ask(question: str, history: List[dict] | None = None) -> Dict[str, Any]:
    """ì™¸ë¶€ì—ì„œ í˜¸ì¶œë˜ëŠ” ì§„ì…ì : ì§ˆë¬¸ì„ ë°›ì•„ ë‹µë³€ + ì†ŒìŠ¤ ë°˜í™˜"""
    t0 = time.time()

    # (1) ê´€ë ¨ ë¬¸ì„œ ë¦¬íŠ¸ë¦¬ë¸Œ
    try:
        retrieved_docs = FILTERED_RETRIEVER.invoke(question)
    except Exception:
        retrieved_docs = RETRIEVER.invoke(question)

    # (2) ë‹µë³€ ìƒì„±
    answer = RAG_CHAIN.invoke(question)

    # (3) ì†ŒìŠ¤ ë¬¸ì„œ ì¼ë¶€ ì¶”ì¶œ
    sources = []
    for d in (retrieved_docs or [])[:5]:
        sources.append({
            "month": d.metadata.get("month"),
            "preview": d.page_content[:120] + "...",
        })

    elapsed = time.time() - t0
    return {"answer": answer, "sources": sources, "usage": {"latency_sec": round(elapsed, 2)}}