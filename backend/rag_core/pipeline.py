# ============================================
# RAG í•µì‹¬ íŒŒì´í”„ë¼ì¸ (Chroma ë²„ì „)
# - ë¬¸ì„œ ë¡œë“œ (ì›¹ + CSV)
# - ì²­í¬ ë¶„í• 
# - ì„ë² ë”© & ë²¡í„°ìŠ¤í† ì–´ (Chroma persist)
# - ë¦¬íŠ¸ë¦¬ë²„ & í”„ë¡¬í”„íŠ¸ & LLM ì²´ì¸ êµ¬ì„±
# - ask() í•¨ìˆ˜ë¡œ ì™¸ë¶€ì— ì œê³µ
# ============================================

from __future__ import annotations
import os, time
from pathlib import Path
from datetime import datetime
from typing import Any, Dict, List, Tuple

from bs4 import BeautifulSoup  
import requests
import pandas as pd
import bs4
from dotenv import load_dotenv

# LangChain ê´€ë ¨ ëª¨ë“ˆ
from langchain import hub
from langchain_core.documents import Document
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.document_loaders import WebBaseLoader
from langchain_chroma import Chroma
from langchain_core.runnables import RunnablePassthrough, RunnableLambda
from langchain_core.output_parsers import StrOutputParser
from langchain_openai import ChatOpenAI, OpenAIEmbeddings

import tiktoken # í† í° ê¸¸ì´ ê³„ì‚°ì— ì‚¬ìš©

# .env ë¡œë“œ (OPENAI_API_KEY, OPENAI_MODEL ë“± í™˜ê²½ ë³€ìˆ˜ ì‚¬ìš©)
load_dotenv()

# ------------------- ê²½ë¡œ ì„¤ì • -------------------
BASE_DIR = Path(__file__).resolve().parent.parent  # backend/
DATA_DIR = BASE_DIR / "data"
CHROMA_DIR = BASE_DIR / "chroma_db"  # Chroma persist ë””ë ‰í„°ë¦¬

# ì„ë² ë”© ëª¨ë¸/ë°°ì¹˜
EMBED_MODEL = os.getenv("OPENAI_EMBED_MODEL", "text-embedding-3-small")
EMBED_BATCH = int(os.getenv("OPENAI_EMBED_BATCH", "16"))  # í•œ ë²ˆì— ë³´ë‚´ëŠ” ë¬¸ì„œ ìˆ˜(ì‘ê²Œ!)
# ì•ˆì „ í† í° ìƒí•œ(ìš”ì²­ë‹¹ 300k ì œí•œë³´ë‹¤ ì—¬ìœ  ìˆê²Œ)
TOKEN_CAP_PER_REQUEST = int(os.getenv("OPENAI_EMBED_TOKEN_CAP", "240000"))

# í† í° ê¸¸ì´ í•¨ìˆ˜ (text-embedding-3-*ì™€ í˜¸í™˜ë˜ëŠ” cl100k_base)
_enc = tiktoken.get_encoding("cl100k_base")
def _tok_len(s: str) -> int:
    return len(_enc.encode(s))

# ------------------- ë°ì´í„° ë¡œë”© -------------------
def _load_web_docs() -> List[Document]:
    """í•œì„±ëŒ€ ë„ì„œê´€ ì›¹ ê·œì • ë¬¸ì„œë¥¼ h4+ul ë‹¨ìœ„ë¡œ ë¡œë“œ"""
    url = "https://hsel.hansung.ac.kr/intro_data.mir"
    resp = requests.get(url)
    soup = BeautifulSoup(resp.text, "html.parser")

    rule_div = soup.find("div", id="intro_rule")
    if not rule_div:
        return []

    docs: List[Document] = []

    # h4 (ì„¹ì…˜ ì œëª©) + ul (ë‚´ìš© ë¦¬ìŠ¤íŠ¸) ì¶”ì¶œ
    for h4 in rule_div.find_all("h4", class_="sub_title"):
        section_title = h4.get_text(strip=True)
        ul = h4.find_next_sibling("ul")
        if ul:
            items = [li.get_text(strip=True) for li in ul.find_all("li")]
            section_text = section_title + " " + " ".join(items)

            docs.append(Document(
                page_content=section_text,
                metadata={"source": "ë„ì„œê´€_ê·œì •", "section": section_title}
            ))

    return docs

def _load_recommend_docs() -> List[Document]:
    """ì¶”ì²œ ê²°ê³¼ CSV(recommend_all.csv)ë¥¼ ë¡œë“œí•˜ì—¬ Document ë¦¬ìŠ¤íŠ¸ ë°˜í™˜"""
    csv = DATA_DIR / "recommend_all.csv"
    df = pd.read_csv(csv, encoding="utf-8")

    docs: List[Document] = []
    for _, row in df.iterrows():
        text = (
            f"[ì„œëª…]{row['ì„œëª…']} | "
            f"[ëŒ€ì¶œíšŸìˆ˜]{row['ëŒ€ì¶œíšŸìˆ˜']} | "
            f"[ëŒ€ì¶œí•™ìƒìˆ˜]{row['ëŒ€ì¶œí•™ìƒìˆ˜']} | "
            f"[í•™ë…„]{row['í•™ë…„']} | "
            f"[í•™ê¸°]{row['í•™ê¸°']} | "
            f"[ë¶„ì•¼]{row['ë¶„ì•¼']}"
        )
        docs.append(Document(page_content=text, metadata={
            "í•™ë…„": row["í•™ë…„"],
            "í•™ê¸°": row["í•™ê¸°"],
            "ë¶„ì•¼": row["ë¶„ì•¼"]
        }))
    return docs

def _extract_title(text: str) -> str:
    if not text:
        return ""
    # ì¼€ì´ìŠ¤ A: [ì„œëª…]ì œëª© | [ëŒ€ì¶œâ€¦] íŒ¨í„´
    m = re.search(r"\[ì„œëª…\]\s*([^|\n]+)", text)
    if m:
        return m.group(1).strip()
    # ì¼€ì´ìŠ¤ B: "ì œëª© | ëŒ€ì¶œâ€¦" í˜•íƒœ
    if " | " in text:
        return text.split(" | ", 1)[0].strip()
    # ì¼€ì´ìŠ¤ C: ê·¸ëƒ¥ í•œ ì¤„ ì œëª©ë§Œ ìˆëŠ” ê²½ìš°
    return text.strip()

# ------------------- ì²­í¬ ë¶„í• (í† í° ê¸°ì¤€) -------------------
def _split_docs(all_docs: List[Document]) -> List[Document]:
    """í† í° ê¸°ì¤€ìœ¼ë¡œ ì•ˆì • ë¶„í• : ìš”ì²­ë‹¹ í† í° ì´ˆê³¼ ë°©ì§€ì— í•µì‹¬"""
    splitter = RecursiveCharacterTextSplitter(
        chunk_size=600,       # â˜… 400~800 ì‚¬ì´ ê¶Œì¥
        chunk_overlap=60,     # â˜… 10% ì •ë„
        length_function=_tok_len,
    )
    return splitter.split_documents(all_docs)

# ------------------- ì•ˆì „ ë°°ì¹˜ ì—…ë¡œë“œ ìœ í‹¸ -------------------
def _batch_indices(n: int, batch_size: int) -> List[Tuple[int, int]]:
    return [(i, min(i + batch_size, n)) for i in range(0, n, batch_size)]

def _build_index_in_safe_batches(vs: Chroma, docs: List[Document], embedding: OpenAIEmbeddings):
    """
    Chroma ì»¬ë ‰ì…˜ì— ë¬¸ì„œë¥¼ 'ì•ˆì „ ë°°ì¹˜'ë¡œ ë‚˜ëˆ  ì¶”ê°€.
    - ë°°ì¹˜ í¬ê¸°(ë¬¸ì„œ ìˆ˜) + ë°°ì¹˜ ë‚´ í† í° ì´í•©ì„ ë™ì‹œì— ì œí•œ
    - OpenAI ì„ë² ë”© APIì˜ ìš”ì²­ë‹¹ 300k í† í° ì œí•œ íšŒí”¼
    """
    texts = [d.page_content for d in docs]
    metadatas = [d.metadata for d in docs]
    n = len(texts)
    print(f"ğŸ“¦ ì•ˆì „ ë°°ì¹˜ ì—…ë¡œë“œ: ì´ {n} ì²­í¬")

    start = 0
    while start < n:
        # 1ì°¨: ë¬¸ì„œ ìˆ˜ ê¸°ì¤€ìœ¼ë¡œ ìë¥´ê¸°
        end = min(start + EMBED_BATCH, n)

        # 2ì°¨: í† í° ìƒí•œ ê³ ë ¤í•´ì„œ end ì¡°ì •
        tok_sum = 0
        safe_end = start
        for i in range(start, end):
            t = texts[i]
            tok = _tok_len(t)
            if tok_sum + tok > TOKEN_CAP_PER_REQUEST:
                break
            tok_sum += tok
            safe_end = i + 1

        # ë§Œì•½ í•œ ë¬¸ì„œê°€ ë„ˆë¬´ ê¸¸ì–´ capì„ ë°”ë¡œ ë„˜ëŠ”ë‹¤ë©´, ê·¸ ë¬¸ì„œë¥¼ ë” ì˜ê²Œ ìª¼ê°œì•¼ í•¨
        if safe_end == start:
            # ë¹„ì •ìƒì ìœ¼ë¡œ ê¸´ ì²­í¬ê°€ ìˆë‹¤ë©´ í•˜ë“œ ì»· (ë§ˆì§€ë§‰ ì•ˆì „ì¥ì¹˜)
            t = texts[start]
            # ì•ìª½ 3000 í† í° ì •ë„ë§Œ ì„ë² ë”©(í•„ìš”í•˜ë©´ ë” ì‘ê²Œ)
            hard_cut = 3000
            truncated = _enc.decode(_enc.encode(t)[:hard_cut])
            vs.add_texts([truncated], metadatas=[metadatas[start]])
            print(f"âš ï¸ ê¸´ ì²­í¬ í•˜ë“œì»· ì²˜ë¦¬ (index {start}, ~{hard_cut} tok)")
            start += 1
            continue

        # ì•ˆì „ êµ¬ê°„ ì¶”ê°€
        vs.add_texts(texts[start:safe_end], metadatas=metadatas[start:safe_end])
        print(f"  â†’ ì—…ë¡œë“œ {start}:{safe_end} (â‰ˆ{tok_sum} tokens)")
        start = safe_end

# ------------------- ë²¡í„°ìŠ¤í† ì–´ ì¤€ë¹„ -------------------
def _get_or_create_vectorstore(splits: List[Document]) -> Chroma:
    embedding = OpenAIEmbeddings(model=EMBED_MODEL)

    if not CHROMA_DIR.exists():
        print("ğŸ”¨ ìƒˆ Chroma ì¸ë±ìŠ¤ ìƒì„± (API í˜¸ì¶œì€ ì´ë²ˆ 1íšŒ)...")
        # ë¹ˆ ì»¬ë ‰ì…˜ì„ ë§Œë“¤ê³  â†’ ì•ˆì „ ë°°ì¹˜ë¡œ add_texts
        vs = Chroma(
            embedding_function=embedding,
            persist_directory=str(CHROMA_DIR)
        )
        _build_index_in_safe_batches(
            vs, splits, embedding)
        vs.persist()
        print("âœ… ì¸ë±ìŠ¤ ìƒì„± & ì €ì¥ ì™„ë£Œ")
    else:
        print("ğŸ“ ê¸°ì¡´ Chroma ì¸ë±ìŠ¤ ë¡œë“œ (API í˜¸ì¶œ ì—†ìŒ)")
        vs = Chroma(embedding_function=embedding,
                     persist_directory=str(CHROMA_DIR))
    return vs

# ------------------- ì „ì—­ ì´ˆê¸°í™” -------------------
print("\nğŸš€ ì¸ë±ìŠ¤ ì¤€ë¹„ ì¤‘...")
web_docs = _load_web_docs()
recommend_docs = _load_recommend_docs()
ALL_DOCS = web_docs + recommend_docs
SPLITS = _split_docs(ALL_DOCS)
print(f"ğŸ“„ ë¬¸ì„œ {len(ALL_DOCS)}ê°œ â†’ ì²­í¬ {len(SPLITS)}ê°œ")

VECTORSTORE = _get_or_create_vectorstore(SPLITS)

# === ë©”íƒ€ë°ì´í„° ì¸ì‹ Custom Reranker ì—…ê·¸ë ˆì´ë“œ ===

from sentence_transformers import CrossEncoder
from langchain_core.documents import Document
from typing import List, Dict, Optional, Tuple
import re
import time

class MetadataAwareCrossEncoderReranker:
    """
    ë©”íƒ€ë°ì´í„°(í•™ë…„, í•™ê¸°, ë¶„ì•¼) ì¡°ê±´ì„ ìš°ì„ í•˜ëŠ” í•˜ì´ë¸Œë¦¬ë“œ Reranker
    - CrossEncoder + ë©”íƒ€ë°ì´í„° ë§¤ì¹­ ì ìˆ˜ ê²°í•©
    - ì¡°ê±´ ì¼ì¹˜ë¥¼ ìµœìš°ì„ ìœ¼ë¡œ ì²˜ë¦¬
    """

    def __init__(self, model_name: str = "cross-encoder/ms-marco-MiniLM-L-6-v2", top_k: int = 3):
        """
        Args: 
            model_name: CrossEncoder ëª¨ë¸ ì´ë¦„
            top_k: ìƒìœ„ ëª‡ ê°œ ë¬¸ì„œë¥¼ ë°˜í™˜í• ì§€
        """
        print(f"ğŸ”„ ë©”íƒ€ë°ì´í„° ì¸ì‹ CrossEncoder ëª¨ë¸ ë¡œë”© ì¤‘: {model_name}")
        self.model = CrossEncoder(model_name)
        self.top_k = top_k
        print(f"âœ… ë©”íƒ€ë°ì´í„° ì¸ì‹ CrossEncoder ë¡œë”© ì™„ë£Œ (top_k={top_k})")

    def extract_conditions(self, query: str) -> Dict[str, Optional[int]]:
        """ì§ˆë¬¸ì—ì„œ í•™ë…„, í•™ê¸°, ë¶„ì•¼ ì¡°ê±´ ì¶”ì¶œ"""
        conditions = {
            'grade': None,
            'semester': None,
            'field': None
        }
        
        # í•™ë…„ ì¶”ì¶œ
        grade_match = re.search(r'(\d+)í•™ë…„', query)
        if grade_match:
            conditions['grade'] = int(grade_match.group(1))
        
        # í•™ê¸° ì¶”ì¶œ  
        semester_match = re.search(r'(\d+)í•™ê¸°', query)
        if semester_match:
            conditions['semester'] = int(semester_match.group(1))
        
        # ë¶„ì•¼ ì¶”ì¶œ
        field_keywords = {
            'ì´ë¥˜': 'ì´ë¥˜',
            'ì² í•™': 'ì² í•™', 
            'ì¢…êµ': 'ì¢…êµ',
            'ì‚¬íšŒê³¼í•™': 'ì‚¬íšŒê³¼í•™',
            'ìì—°ê³¼í•™': 'ìì—°ê³¼í•™',
            'ê¸°ìˆ ê³¼í•™': 'ê¸°ìˆ ê³¼í•™',
            'ì˜ˆìˆ ': 'ì˜ˆìˆ ',
            'ì–¸ì–´': 'ì–¸ì–´',
            'ë¬¸í•™': 'ë¬¸í•™',
            'ì—­ì‚¬': 'ì—­ì‚¬'
        }
        
        for keyword, field in field_keywords.items():
            if keyword in query:
                conditions['field'] = field
                break
        
        return conditions

    def calculate_metadata_score(self, metadata: dict, conditions: dict) -> Tuple[int, list]:
        """ë©”íƒ€ë°ì´í„° ë§¤ì¹­ ì ìˆ˜ ê³„ì‚°"""
        score = 0
        reasons = []
        
        # í•™ë…„ ë§¤ì¹­ (ë†’ì€ ê°€ì¤‘ì¹˜)
        if conditions['grade'] and metadata.get('í•™ë…„') == conditions['grade']:
            score += 1000  
            reasons.append(f"í•™ë…„ ì¼ì¹˜({conditions['grade']})")
        
        # í•™ê¸° ë§¤ì¹­ (ë†’ì€ ê°€ì¤‘ì¹˜)
        if conditions['semester'] and metadata.get('í•™ê¸°') == conditions['semester']:
            score += 1000  
            reasons.append(f"í•™ê¸° ì¼ì¹˜({conditions['semester']})")
            
        # ë¶„ì•¼ ë§¤ì¹­ (ì¤‘ê°„ ê°€ì¤‘ì¹˜)
        if conditions['field'] and metadata.get('ë¶„ì•¼') == conditions['field']:
            score += 1200
            reasons.append(f"ë¶„ì•¼ ì¼ì¹˜({conditions['field']})")
        
        return score, reasons

    def rerank_documents(self, documents: List[Document], query: str) -> List[Document]:
        """
        ë©”íƒ€ë°ì´í„° ìš°ì„  + CrossEncoder í•˜ì´ë¸Œë¦¬ë“œ ë¦¬ë­í‚¹
        """
        if not documents:
            return documents
        
        if len(documents) <= self.top_k:
            return documents

        # ì¡°ê±´ ì¶”ì¶œ
        conditions = self.extract_conditions(query)
        print(f"ğŸ¯ ì¶”ì¶œëœ ì¡°ê±´: í•™ë…„={conditions['grade']}, í•™ê¸°={conditions['semester']}, ë¶„ì•¼={conditions['field']}")
        
        # ì¿¼ë¦¬-ë¬¸ì„œ ìŒ ìƒì„±
        pairs = []
        for doc in documents:
            content = doc.page_content[:512] # 512ì ì œí•œ
            pairs.append((query, content))

        # CrossEncoder ì ìˆ˜ ê³„ì‚°
        print(f"ğŸ”„ {len(pairs)}ê°œ ë¬¸ì„œ í•˜ì´ë¸Œë¦¬ë“œ ë¦¬ë­í‚¹ ì¤‘...")
        cross_scores = self.model.predict(pairs)

        # í•˜ì´ë¸Œë¦¬ë“œ ì ìˆ˜ ê³„ì‚°
        scored_docs = []
        
        for i, doc in enumerate(documents):
            cross_score = float(cross_scores[i])
            metadata_score, reasons = self.calculate_metadata_score(doc.metadata, conditions)
            
            # ìµœì¢… ì ìˆ˜ = ë©”íƒ€ë°ì´í„° ì ìˆ˜ (ìš°ì„ ) + CrossEncoder ì ìˆ˜ (ë³´ì¡°)
            final_score = metadata_score + cross_score
            
            title = _extract_title(doc.page_content)
            
            scored_docs.append({
                'document': doc,
                'final_score': final_score,
                'metadata_score': metadata_score,
                'cross_score': cross_score,
                'reasons': reasons,
                'title': title
            })

        # ìµœì¢… ì ìˆ˜ë¡œ ì •ë ¬
        scored_docs.sort(key=lambda x: x['final_score'], reverse=True)

        print(f" âœ… í•˜ì´ë¸Œë¦¬ë“œ ë¦¬ë­í‚¹ ì™„ë£Œ: {len(documents)} -> {self.top_k}ê°œ ë¬¸ì„œ")

        # ë””ë²„ê¹…: ìƒìœ„ ë¬¸ì„œë“¤ì˜ ì ìˆ˜ ì¶œë ¥
        for i, item in enumerate(scored_docs[:self.top_k]):
            reasons_str = ', '.join(item['reasons']) if item['reasons'] else 'ì¡°ê±´ ë¶ˆì¼ì¹˜'
            print(f" {i+1}. {item['title'][:40]}...")
            print(f"    ìµœì¢…ì ìˆ˜: {item['final_score']:.1f} = ë©”íƒ€ë°ì´í„°({item['metadata_score']}) + CrossEncoder({item['cross_score']:.3f})")
            print(f"    ë§¤ì¹­: {reasons_str}")

        # ìƒìœ„ kê°œ ë¬¸ì„œë§Œ ë°˜í™˜
        top_docs = [item['document'] for item in scored_docs[:self.top_k]]
        
        return top_docs
    
class MetadataAwareRetriever:
    """
    ë©”íƒ€ë°ì´í„° ì¸ì‹ Rerankerê°€ ì ìš©ëœ Retriever
    """

    def __init__(self, vectorstore, reranker: MetadataAwareCrossEncoderReranker, initial_k: int = 15):
        """ì´ˆê¸° ê²€ìƒ‰ì„ ë” ë§ì´ í•´ì„œ ëˆ„ë½ ë°©ì§€"""
        self.base_retriever = vectorstore.as_retriever(
            search_type="mmr",
            search_kwargs={"k": initial_k}
        )
        self.reranker = reranker
        self.initial_k = initial_k
        print(f"âœ… ë©”íƒ€ë°ì´í„° ì¸ì‹ Retriever ì„¤ì • ì™„ë£Œ (ì´ˆê¸° ê²€ìƒ‰: {initial_k}ê°œ)")
    
    def invoke(self, query: str) -> List[Document]:
        """ë©”íƒ€ë°ì´í„° ìš°ì„  ê²€ìƒ‰ + ë¦¬ë­í‚¹"""
        # 1ë‹¨ê³„: ë” ë§ì€ ë¬¸ì„œ ê²€ìƒ‰ (ëˆ„ë½ ë°©ì§€)
        initial_docs = self.base_retriever.invoke(query)

        # 2ë‹¨ê³„: ë©”íƒ€ë°ì´í„° + CrossEncoder í•˜ì´ë¸Œë¦¬ë“œ ë¦¬ë­í‚¹
        reranked_docs = self.reranker.rerank_documents(initial_docs, query)

        return reranked_docs
    
    def get_relevant_documents(self, query: str) -> List[Document]:
        """Legacy í˜¸í™˜ì„±"""
        return self.invoke(query)
    
# === ê¸°ì¡´ Custom Rerankerë¥¼ ë©”íƒ€ë°ì´í„° ì¸ì‹ ë²„ì „ìœ¼ë¡œ êµì²´ ===
print("\nğŸš€ ë©”íƒ€ë°ì´í„° ì¸ì‹ CrossEncoder Rerankerë¡œ ì—…ê·¸ë ˆì´ë“œ ì¤‘...")

# ë©”íƒ€ë°ì´í„° ì¸ì‹ Reranker ìƒì„±
metadata_aware_reranker = MetadataAwareCrossEncoderReranker(
    model_name="cross-encoder/ms-marco-MiniLM-L-6-v2",
    top_k=3
)

# ì±… ì¶”ì²œ ê´€ë ¨ RETRIEVERë¥¼ Custom Rerankerë¡œ êµì²´
Book_RETRIEVER = MetadataAwareRetriever(
    vectorstore=VECTORSTORE,
    reranker=metadata_aware_reranker,
    initial_k=15
)

# ë„ì„œê´€ ê·œì • ê´€ë ¨ ë¦¬íŠ¸ë¦¬ë²„ (ì˜ë¯¸ì¶”ë¡ ë§Œ ì ìš©ëœ ê¸°ë³¸ ìƒíƒœ)
Library_RETRIEVER = VECTORSTORE.as_retriever(
    search_type="mmr", search_kwargs={"k": 5}
)

print("âœ… Custom Reranker íŒŒì´í”„ë¼ì¸ ì™„ë£Œ!")


# ì¿¼ë¦¬ ë¶„ë¥˜ (ì‚¬ìš©ìì˜ ì§ˆë¬¸ ë¶„ë¦¬í•˜ê¸°)
import re
from enum import Enum

class QueryType(Enum):
    BOOK_RECOMMENDATION = "book"
    LIBRARY_INFO = "library"

def classify_simple(query: str) -> QueryType:
    """ê°„ë‹¨í•œ ì¿¼ë¦¬ ë¶„ë¥˜"""
    book_keywords = ['ì¶”ì²œ', 'í•™ë…„', 'í•™ê¸°', 'ë¶„ì•¼']
    if any(keyword in query for keyword in book_keywords):
        return QueryType.BOOK_RECOMMENDATION
    return QueryType.LIBRARY_INFO

# í”„ë¡¬í”„íŠ¸ & LLM ì¤€ë¹„
from langchain_core.prompts import PromptTemplate

BOOK_PROMPT = PromptTemplate.from_template(
        """
ë‹¹ì‹ ì€ í•œì„±ëŒ€í•™êµ í•™ìˆ ì •ë³´ê´€ ë„ì„œ ì¶”ì²œ ì „ë¬¸ ì±—ë´‡ì…ë‹ˆë‹¤.

ğŸš¨ **ì›ì¹™:**
1. ì˜¤ì§ ì•„ë˜ Contextì— ìˆëŠ” ë„ì„œë§Œ í™œìš©í•˜ì„¸ìš”
2. Context ë°–ì˜ ë„ì„œëŠ” ì ˆëŒ€ ìƒì„±í•˜ì§€ ë§ˆì„¸ìš”
3. ë„ì„œëª…ì€ ë°˜ë“œì‹œ Contextì—ì„œ ì •í™•íˆ ë³µì‚¬í•˜ì„¸ìš”

**Context (ê´€ë ¨ì„± ì ìˆ˜ ìˆœ):**
{context}

**ì§ˆë¬¸:** {question}

**ì¶œë ¥ í˜•ì‹:**

ğŸ“š **ì¶”ì²œ ë„ì„œ**: [ë„ì„œëª…]

ğŸ¯ **ì¶”ì²œ ì´ìœ **: í•´ë‹¹ í•™ë…„Â·í•™ê¸°Â·ë¶„ì•¼ì—ì„œ í•™ìƒë“¤ì—ê²Œ ê°€ì¥ ì¸ê¸° ìˆëŠ” ë„ì„œì…ë‹ˆë‹¤.

ğŸ“– **ë„ì„œ ì†Œê°œ**: [í•´ë‹¹ ë„ì„œì˜ ì£¼ìš” ë‚´ìš©ì´ë‚˜ íŠ¹ì§•ì„ 2-3ë¬¸ì¥ìœ¼ë¡œ ì„¤ëª…. ë„ì„œëª…ì„ ë°”íƒ•ìœ¼ë¡œ ììœ ë¡­ê³  ì°½ì˜ì ì´ê²Œ ì„¤ëª… ì œê³µ]

ğŸ“ **í•™ìŠµ íš¨ê³¼**: [ì´ ì±…ì„ í†µí•´ ì–»ì„ ìˆ˜ ìˆëŠ” ì§€ì‹/ìŠ¤í‚¬ì„ êµ¬ì²´ì ìœ¼ë¡œ]

â­ **ì¶”ì²œ ëŒ€ìƒ**: [ì–´ë–¤ í•™ìƒë“¤ì—ê²Œ íŠ¹íˆ ìœ ìš©í•œì§€ )]

ğŸ’¡ **ì°¸ê³ **: ì»´í“¨í„°ê³µí•™ë¶€ í•™ìƒë“¤ì˜ ì‹¤ì œ 10ë…„ì¹˜ ëŒ€ì¶œ ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•œ ì¶”ì²œì…ë‹ˆë‹¤.
"""
)

LIBRARY_PROMPT = PromptTemplate.from_template(
    """
ë‹¹ì‹ ì€ í•œì„±ëŒ€í•™êµ í•™ìˆ ì •ë³´ê´€ ì•ˆë‚´ ì±—ë´‡ì…ë‹ˆë‹¤.

ì•„ë˜ ì •ë³´ë¥¼ ì°¸ê³ í•˜ì—¬ ì¹œì ˆí•˜ê²Œ ë‹µë³€í•´ì£¼ì„¸ìš”.

**ì°¸ê³  ì •ë³´:**
{context}

**ì§ˆë¬¸:** {question}

**ë‹µë³€:**
"""
)

MODEL_NAME = os.getenv("OPENAI_MODEL", "gpt-4o-mini")
LLM = ChatOpenAI(model=MODEL_NAME, temperature=0)

print("ğŸ¯ RAG íŒŒì´í”„ë¼ì¸ ì¤€ë¹„ ì™„ë£Œ!\n")
import re

def validate_book_answer(answer: str, retrieved_docs: list) -> str:
    """ë‹¨ì¼ ì±… ì¶”ì²œ ë‹µë³€ ê²€ì¦ ë° êµì •"""
    
    # ì»¨í…ìŠ¤íŠ¸ì—ì„œ ì±… ì œëª©ë“¤ ì¶”ì¶œ
    context_titles = []
    for doc in retrieved_docs:
        title = _extract_title(getattr(doc, "page_content", str(doc)))
        if title and title not in context_titles:
            context_titles.append(title)
    
    print(f"ğŸ” ì»¨í…ìŠ¤íŠ¸ì—ì„œ ì¶”ì¶œëœ ì±…ë“¤: {context_titles[:3]}")  # ë””ë²„ê¹…ìš©
    
    # ë‹µë³€ì—ì„œ ì‹¤ì œ ì»¨í…ìŠ¤íŠ¸ ì œëª©ì´ í¬í•¨ë˜ì—ˆëŠ”ì§€ í™•ì¸
    found_title = None
    for title in context_titles:
        if title in answer:
            found_title = title
            break
    
    if found_title:
        # ì˜¬ë°”ë¥¸ ì±…ì´ í¬í•¨ëœ ê²½ìš° - ë‹µë³€ ê·¸ëŒ€ë¡œ ë°˜í™˜í•˜ê±°ë‚˜ ì •ë¦¬
        print(f"âœ… ì˜¬ë°”ë¥¸ ì±… ë°œê²¬: {found_title}")
        return answer
    else:
        # ì»¨í…ìŠ¤íŠ¸ì— ì—†ëŠ” ì±…ì´ ì¶”ì²œëœ ê²½ìš° - ì²« ë²ˆì§¸ ì±…ìœ¼ë¡œ ëŒ€ì²´
        if context_titles:
            best_title = context_titles[0]  # ê²€ìƒ‰ ê²°ê³¼ ìƒìœ„ ì±…
            corrected_answer = f"""ì¶”ì²œ ê¸°ì¤€: ì§ˆë¬¸ ì£¼ì‹  í•´ë‹¹ í•™ë…„ê³¼ í•™ê¸°ì— ëŒ€ì¶œëœ ë‚´ì—­ì„ ê¸°ë°˜ìœ¼ë¡œ ê°€ì¥ ë§ì´ ëŒ€ì¶œëœ ë„ì„œë¥¼ ì¶”ì²œí•´ ë“œë¦½ë‹ˆë‹¤.

ì¶”ì²œ ë„ì„œ: {best_title}"""
            print(f"ğŸ”§ ë‹µë³€ êµì •: ì»¨í…ìŠ¤íŠ¸ ì²« ë²ˆì§¸ ì±…ìœ¼ë¡œ ëŒ€ì²´ â†’ {best_title}")
            return corrected_answer
        else:
            return "ì£„ì†¡í•©ë‹ˆë‹¤. í˜„ì¬ ì¡°ê±´ì— ë§ëŠ” ì ì ˆí•œ ë„ì„œë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."


# ------------------- ê³µê°œ í•¨ìˆ˜ -------------------
def ask(question: str, history: List[dict] | None = None) -> Dict[str, Any]:
    """ì™¸ë¶€ì—ì„œ í˜¸ì¶œë˜ëŠ” ì§„ì…ì : ì§ˆë¬¸ì„ ë°›ì•„ ì¿¼ë¦¬ íƒ€ì…ë³„ ì²˜ë¦¬ + ì†ŒìŠ¤ ë°˜í™˜"""
    t0 = time.time()

    # ì¿¼ë¦¬ ë¶„ë¥˜
    query_type = classify_simple(question)

    # íƒ€ì…ë³„ ë¦¬íŠ¸ë¦¬ë²„ & í”„ë¡¬í”„íŠ¸ ì„ íƒ
    if query_type == QueryType.BOOK_RECOMMENDATION:
        retriever = Book_RETRIEVER
        prompt = BOOK_PROMPT
    else:
        retriever = Library_RETRIEVER
        prompt = LIBRARY_PROMPT

    retrieved_docs = retriever.invoke(question)

    # Contextë¥¼ ìˆ˜ë™ìœ¼ë¡œ êµ¬ì„± (Custom Retriever ì‚¬ìš© ì‹œ í•„ìš”)
    context = "\n\n".join([doc.page_content for doc in retrieved_docs])
    
    chain = prompt | LLM | StrOutputParser()

    # (2) ë‹µë³€ ìƒì„± - ìˆ˜ë™ìœ¼ë¡œ contextì™€ question ì „ë‹¬
    answer = chain.invoke({
        "context": context,
        "question": question
    })

    # ë‹µë³€ í›„ì²˜ë¦¬ (í• ë£¨ì‹œë„¤ì´ì…˜ ì²´í¬)
    if query_type == QueryType.BOOK_RECOMMENDATION:
        answer = validate_book_answer(answer, retrieved_docs)

    # (3) ì†ŒìŠ¤ ë¬¸ì„œ ì¼ë¶€ ì¶”ì¶œ
    sources = []
    for d in (retrieved_docs or [])[:5]:
        sources.append({
        "í•™ë…„": d.metadata.get("í•™ë…„"),
        "í•™ê¸°": d.metadata.get("í•™ê¸°"),
        "ë¶„ì•¼": d.metadata.get("ë¶„ì•¼"),
        "preview": d.page_content[:120] + "..."
    })

    elapsed = time.time() - t0
    return {"answer": answer, "sources": sources, "usage": {"latency_sec": round(elapsed, 2)}}

# í…ŒìŠ¤íŠ¸ ì‹¤í–‰
if __name__ == "__main__":
    question = "4í•™ë…„ 1í•™ê¸° ê¸°ìˆ ê³¼í•™ ë¶„ì•¼ì—ì„œ ì¶”ì²œí•  ë„ì„œëŠ”?"
    print(f"ğŸ“ ì§ˆë¬¸: {question}")
    
    try:
        result = ask(question)
        print(f"ğŸ¤– ë‹µë³€:\n{result['answer']}")
        print(f"â±ï¸ ì‘ë‹µì‹œê°„: {result['usage']['latency_sec']}ì´ˆ")
        print(f"ğŸ“š ê²€ìƒ‰ëœ ë¬¸ì„œ ìˆ˜: {len(result['sources'])}")
    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜: {e}")