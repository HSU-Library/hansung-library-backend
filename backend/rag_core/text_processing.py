# ===== rag_core/text_processing.py - í…ìŠ¤íŠ¸ ì²˜ë¦¬ =====
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_chroma import Chroma
from langchain_openai import OpenAIEmbeddings
from langchain_core.documents import Document
from typing import List, Tuple
from .config import _tok_len, EMBED_BATCH, TOKEN_CAP_PER_REQUEST, EMBED_MODEL, _enc

def _split_docs(all_docs: List[Document]) -> List[Document]:
    """í† í° ê¸°ì¤€ìœ¼ë¡œ ì•ˆì • ë¶„í• : ìš”ì²­ë‹¹ í† í° ì´ˆê³¼ ë°©ì§€ì— í•µì‹¬"""
    splitter = RecursiveCharacterTextSplitter(
        chunk_size=600,       # â˜… 400~800 ì‚¬ì´ ê¶Œì¥
        chunk_overlap=60,     # â˜… 10% ì •ë„
        length_function=_tok_len,
    )
    return splitter.split_documents(all_docs)

def _batch_indices(n: int, batch_size: int) -> List[Tuple[int, int]]:
    return [(i, min(i + batch_size, n)) for i in range(0, n, batch_size)]

def _build_index_in_safe_batches(vs: Chroma, docs: List[Document], embedding: OpenAIEmbeddings):
    """
    Chroma ì»¬ë ‰ì…˜ì— ë¬¸ì„œë¥¼ 'ì•ˆì „ ë°°ì¹˜'ë¡œ ë‚˜ëˆ  ì¶”ê°€.
    - ë°°ì¹˜ í¬ê¸°(ë¬¸ì„œ ìˆ˜) + ë°°ì¹˜ ë‚´ í† í° ì´í•©ì„ ë™ì‹œì— ì œí•œ
    - OpenAI ì„ë² ë”© APIì˜ ìš”ì²­ë‹¹ 300k í† í° ì œí•œ íšŒí”¼
    """
    texts = [d.page_content for d in docs]
    metadatas = [d.metadata for d in docs]
    n = len(texts)
    print(f"ğŸ“¦ ì•ˆì „ ë°°ì¹˜ ì—…ë¡œë“œ: ì´ {n} ì²­í¬")

    start = 0
    while start < n:
        # 1ì°¨: ë¬¸ì„œ ìˆ˜ ê¸°ì¤€ìœ¼ë¡œ ìë¥´ê¸°
        end = min(start + EMBED_BATCH, n)

        # 2ì°¨: í† í° ìƒí•œ ê³ ë ¤í•´ì„œ end ì¡°ì •
        tok_sum = 0
        safe_end = start
        for i in range(start, end):
            t = texts[i]
            tok = _tok_len(t)
            if tok_sum + tok > TOKEN_CAP_PER_REQUEST:
                break
            tok_sum += tok
            safe_end = i + 1

        # ë§Œì•½ í•œ ë¬¸ì„œê°€ ë„ˆë¬´ ê¸¸ì–´ capì„ ë°”ë¡œ ë„˜ëŠ”ë‹¤ë©´, ê·¸ ë¬¸ì„œë¥¼ ë” ì˜ê²Œ ìª¼ê°œì•¼ í•¨
        if safe_end == start:
            # ë¹„ì •ìƒì ìœ¼ë¡œ ê¸´ ì²­í¬ê°€ ìˆë‹¤ë©´ í•˜ë“œ ì»· (ë§ˆì§€ë§‰ ì•ˆì „ì¥ì¹˜)
            t = texts[start]
            # ì•ìª½ 3000 í† í° ì •ë„ë§Œ ì„ë² ë”©(í•„ìš”í•˜ë©´ ë” ì‘ê²Œ)
            hard_cut = 3000
            truncated = _enc.decode(_enc.encode(t)[:hard_cut])
            vs.add_texts([truncated], metadatas=[metadatas[start]])
            print(f"âš ï¸ ê¸´ ì²­í¬ í•˜ë“œì»· ì²˜ë¦¬ (index {start}, ~{hard_cut} tok)")
            start += 1
            continue

        # ì•ˆì „ êµ¬ê°„ ì¶”ê°€
        vs.add_texts(texts[start:safe_end], metadatas=metadatas[start:safe_end])
        print(f"  â†’ ì—…ë¡œë“œ {start}:{safe_end} (â‰ˆ{tok_sum} tokens)")
        start = safe_end

def _get_or_create_vectorstore(splits: List[Document]) -> Chroma:
    from .config import CHROMA_DIR
    embedding = OpenAIEmbeddings(model=EMBED_MODEL)

    if not CHROMA_DIR.exists():
        print("ğŸ”¨ ìƒˆ Chroma ì¸ë±ìŠ¤ ìƒì„± (API í˜¸ì¶œì€ ì´ë²ˆ 1íšŒ)...")
        # ë¹ˆ ì»¬ë ‰ì…˜ì„ ë§Œë“¤ê³  â†’ ì•ˆì „ ë°°ì¹˜ë¡œ add_texts
        vs = Chroma(
            embedding_function=embedding,
            persist_directory=str(CHROMA_DIR)
        )
        _build_index_in_safe_batches(
            vs, splits, embedding)
        vs.persist()
        print("âœ… ì¸ë±ìŠ¤ ìƒì„± & ì €ì¥ ì™„ë£Œ")
    else:
        print("ğŸ“ ê¸°ì¡´ Chroma ì¸ë±ìŠ¤ ë¡œë“œ (API í˜¸ì¶œ ì—†ìŒ)")
        vs = Chroma(embedding_function=embedding,
                     persist_directory=str(CHROMA_DIR))
    return vs
